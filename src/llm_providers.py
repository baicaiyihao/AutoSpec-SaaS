"""
LLM Providers - ç»Ÿä¸€çš„å¤šæ¨¡å‹æ¥å£

æ”¯æŒçš„æ¨¡å‹æä¾›å•†:
- OpenAI (GPT-4, GPT-4o, o1)
- Anthropic (Claude 3.5, Claude 4)
- Google (Gemini 2.0, Gemini 2.5)
- DeepSeek (DeepSeek-V3, DeepSeek-R1)
- ZhipuAI (GLM-4, GLM-4V)
- Alibaba DashScope (Qwen-Max, Qwen-Plus)
- Ollama (æœ¬åœ°æ¨¡å‹)

è®¾è®¡åŸåˆ™:
1. ç»Ÿä¸€æ¥å£ - æ‰€æœ‰Providerå®ç°ç›¸åŒçš„BaseLLMProvider
2. å»¶è¿Ÿåˆå§‹åŒ– - åªåœ¨éœ€è¦æ—¶åˆ›å»ºå®¢æˆ·ç«¯
3. é…ç½®é©±åŠ¨ - é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ç®¡ç†APIå¯†é’¥
4. å®¹é”™è®¾è®¡ - æ”¯æŒfallbackåˆ°å¤‡ç”¨æ¨¡å‹
"""

import os
import json
from pathlib import Path
from abc import ABC, abstractmethod

# åŠ è½½ .env æ–‡ä»¶
from dotenv import load_dotenv
load_dotenv(Path(__file__).resolve().parent.parent.parent / ".env")
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union
from enum import Enum


class ProviderType(Enum):
    """æ¨¡å‹æä¾›å•†ç±»å‹"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    DEEPSEEK = "deepseek"
    ZHIPU = "zhipu"
    DASHSCOPE = "dashscope"
    OLLAMA = "ollama"
    OPENAI_COMPATIBLE = "openai_compatible"  # å…¼å®¹OpenAI APIçš„æœåŠ¡


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    provider: ProviderType
    model: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    temperature: float = 0.1
    max_tokens: int = 4096
    timeout: int = 120
    extra_params: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨è¯·æ±‚"""
    id: str
    name: str
    arguments: Dict[str, Any]


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    content: str
    model: str
    usage: Dict[str, int] = field(default_factory=dict)
    raw_response: Any = None
    tool_calls: List[ToolCall] = field(default_factory=list)  # ğŸ”¥ AI è¯·æ±‚çš„å·¥å…·è°ƒç”¨
    finish_reason: str = "stop"  # "stop" | "tool_calls" | "length"


class BaseLLMProvider(ABC):
    """
    LLMæä¾›å•†åŸºç±»

    æ‰€æœ‰æä¾›å•†å¿…é¡»å®ç°æ­¤æ¥å£ã€‚
    """

    def __init__(self, config: LLMConfig):
        self.config = config
        self._client = None

    @property
    def client(self):
        """å»¶è¿Ÿåˆå§‹åŒ–å®¢æˆ·ç«¯"""
        if self._client is None:
            self._client = self._create_client()
        return self._client

    @abstractmethod
    def _create_client(self) -> Any:
        """åˆ›å»ºåº•å±‚å®¢æˆ·ç«¯"""
        pass

    @abstractmethod
    def chat(
        self,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> LLMResponse:
        """
        å‘é€èŠå¤©è¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            LLMResponse
        """
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨"""
        pass

    def invoke(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        """å…¼å®¹langchainçš„invokeæ–¹æ³•"""
        return self.chat(messages, **kwargs)


# =============================================================================
# OpenAI Provider
# =============================================================================

class OpenAIProvider(BaseLLMProvider):
    """OpenAI API (GPT-4, GPT-4o, o1ç­‰) - æ”¯æŒ Function Calling"""

    def _create_client(self):
        try:
            from openai import OpenAI
            api_key = self.config.api_key or os.getenv("OPENAI_API_KEY")
            return OpenAI(
                api_key=api_key,
                base_url=self.config.base_url,
                timeout=self.config.timeout
            )
        except ImportError:
            raise ImportError("è¯·å®‰è£…openai: pip install openai")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": self.config.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            **self.config.extra_params
        }

        # ğŸ”¥ æ·»åŠ å·¥å…·å®šä¹‰ (å¦‚æœæä¾›)
        tools = kwargs.get("tools")
        if tools:
            request_params["tools"] = tools
            # å…è®¸ AI é€‰æ‹©æ˜¯å¦è°ƒç”¨å·¥å…·
            request_params["tool_choice"] = kwargs.get("tool_choice", "auto")

        response = self.client.chat.completions.create(**request_params)

        # è§£æå“åº”
        message = response.choices[0].message
        finish_reason = response.choices[0].finish_reason

        # ğŸ”¥ è§£æå·¥å…·è°ƒç”¨
        tool_calls = []
        if message.tool_calls:
            for tc in message.tool_calls:
                try:
                    args = json.loads(tc.function.arguments) if tc.function.arguments else {}
                except json.JSONDecodeError:
                    args = {}
                tool_calls.append(ToolCall(
                    id=tc.id,
                    name=tc.function.name,
                    arguments=args
                ))

        return LLMResponse(
            content=message.content or "",
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            raw_response=response,
            tool_calls=tool_calls,
            finish_reason=finish_reason
        )

    def is_available(self) -> bool:
        api_key = self.config.api_key or os.getenv("OPENAI_API_KEY")
        return bool(api_key)


# =============================================================================
# Anthropic Provider
# =============================================================================

class AnthropicProvider(BaseLLMProvider):
    """
    Anthropic API (Claudeç³»åˆ—)

    å®‰è£…: pip install anthropic
    æ¨¡å‹: claude-sonnet-4-5, claude-sonnet-4, claude-opus-4, claude-haiku-4-5 ç­‰
    """

    def _create_client(self):
        try:
            import anthropic
            api_key = self.config.api_key or os.getenv("ANTHROPIC_API_KEY")
            return anthropic.Anthropic(api_key=api_key)
        except ImportError:
            raise ImportError("è¯·å®‰è£…anthropic: pip install anthropic")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        # åˆ†ç¦»system message
        system = None
        chat_messages = []
        for msg in messages:
            if msg["role"] == "system":
                system = msg["content"]
            else:
                chat_messages.append({"role": msg["role"], "content": msg["content"]})

        # æ„å»ºè¯·æ±‚å‚æ•°
        create_params = {
            "model": self.config.model or "claude-sonnet-4-5",
            "messages": chat_messages,
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
        }

        # system æ˜¯å¯é€‰çš„
        if system:
            create_params["system"] = system

        # temperature å¯¹æŸäº›æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ
        if self.config.temperature > 0:
            create_params["temperature"] = kwargs.get("temperature", self.config.temperature)

        response = self.client.messages.create(**create_params)

        # æå–æ–‡æœ¬å†…å®¹
        content = ""
        for block in response.content:
            if hasattr(block, 'text'):
                content += block.text

        return LLMResponse(
            content=content,
            model=response.model,
            usage={
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
                "total_tokens": response.usage.input_tokens + response.usage.output_tokens
            },
            raw_response=response
        )

    def is_available(self) -> bool:
        api_key = self.config.api_key or os.getenv("ANTHROPIC_API_KEY")
        return bool(api_key)


# =============================================================================
# Google Gemini Provider
# =============================================================================

class GoogleProvider(BaseLLMProvider):
    """
    Google Gemini API (ä½¿ç”¨ google-genai SDK)

    å®‰è£…: pip install google-genai
    æ¨¡å‹: gemini-3-flash, gemini-3-pro, gemini-2.5-pro, gemini-2.5-flash ç­‰
    """

    def _create_client(self):
        try:
            from google import genai
            api_key = self.config.api_key or os.getenv("GOOGLE_API_KEY")
            return genai.Client(api_key=api_key)
        except ImportError:
            raise ImportError("è¯·å®‰è£…google-genai: pip install google-genai")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º contents
        contents = []
        system_instruction = None

        for msg in messages:
            if msg["role"] == "system":
                system_instruction = msg["content"]
            elif msg["role"] == "user":
                contents.append({"role": "user", "parts": [{"text": msg["content"]}]})
            elif msg["role"] == "assistant":
                contents.append({"role": "model", "parts": [{"text": msg["content"]}]})

        # æ„å»ºè¯·æ±‚å‚æ•°
        generate_params = {
            "model": self.config.model or "gemini-3-flash",
            "contents": contents,
        }

        # æ·»åŠ ç”Ÿæˆé…ç½®
        config = {
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_output_tokens": kwargs.get("max_tokens", self.config.max_tokens),
        }
        if system_instruction:
            config["system_instruction"] = system_instruction

        generate_params["config"] = config

        response = self.client.models.generate_content(**generate_params)

        return LLMResponse(
            content=response.text,
            model=self.config.model,
            usage={
                "prompt_tokens": getattr(response.usage_metadata, 'prompt_token_count', 0) if hasattr(response, 'usage_metadata') else 0,
                "completion_tokens": getattr(response.usage_metadata, 'candidates_token_count', 0) if hasattr(response, 'usage_metadata') else 0,
                "total_tokens": getattr(response.usage_metadata, 'total_token_count', 0) if hasattr(response, 'usage_metadata') else 0,
            },
            raw_response=response
        )

    def is_available(self) -> bool:
        api_key = self.config.api_key or os.getenv("GOOGLE_API_KEY")
        return bool(api_key)


# =============================================================================
# DeepSeek Provider
# =============================================================================

class DeepSeekProvider(BaseLLMProvider):
    """DeepSeek API (å…¼å®¹OpenAIæ ¼å¼)"""

    def _create_client(self):
        try:
            from openai import OpenAI
            api_key = self.config.api_key or os.getenv("DEEPSEEK_API_KEY")
            return OpenAI(
                api_key=api_key,
                base_url=self.config.base_url or "https://api.deepseek.com/v1",
                timeout=self.config.timeout
            )
        except ImportError:
            raise ImportError("è¯·å®‰è£…openai: pip install openai")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        response = self.client.chat.completions.create(
            model=self.config.model or "deepseek-chat",
            messages=messages,
            temperature=kwargs.get("temperature", self.config.temperature),
            max_tokens=kwargs.get("max_tokens", self.config.max_tokens),
            **self.config.extra_params
        )
        return LLMResponse(
            content=response.choices[0].message.content,
            model=response.model,
            usage={
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            raw_response=response
        )

    def is_available(self) -> bool:
        api_key = self.config.api_key or os.getenv("DEEPSEEK_API_KEY")
        return bool(api_key)


# =============================================================================
# ZhipuAI (GLM) Provider
# =============================================================================

class ZhipuProvider(BaseLLMProvider):
    """
    æ™ºè°±AI GLM API (ä½¿ç”¨ zai-sdk)

    å®‰è£…: pip install zai-sdk
    æ¨¡å‹: glm-4.7, glm-4-plus, glm-4-long ç­‰
    """

    def _create_client(self):
        try:
            from zai import ZhipuAiClient
            api_key = self.config.api_key or os.getenv("ZHIPU_API_KEY")
            # ğŸ”¥ ç¦ç”¨ SDK å†…éƒ¨é‡è¯•ï¼Œç”±æˆ‘ä»¬çš„ BaseAgent.call_llm ç»Ÿä¸€å¤„ç†é‡è¯•
            return ZhipuAiClient(api_key=api_key, max_retries=0)
        except ImportError:
            raise ImportError("è¯·å®‰è£…zai-sdk: pip install zai-sdk")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        call_params = {
            "model": self.config.model or "glm-4.7",
            "messages": messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
        }

        response = self.client.chat.completions.create(**call_params)

        # æå–å†…å®¹
        message = response.choices[0].message
        content = message.content or ""

        return LLMResponse(
            content=content,
            model=response.model if hasattr(response, 'model') else self.config.model,
            usage={
                "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0) if hasattr(response, 'usage') else 0,
                "completion_tokens": getattr(response.usage, 'completion_tokens', 0) if hasattr(response, 'usage') else 0,
                "total_tokens": getattr(response.usage, 'total_tokens', 0) if hasattr(response, 'usage') else 0
            },
            raw_response=response
        )

    def is_available(self) -> bool:
        api_key = self.config.api_key or os.getenv("ZHIPU_API_KEY")
        return bool(api_key)


# =============================================================================
# DashScope Provider (é˜¿é‡Œäº‘) - ä½¿ç”¨ LangChain ChatTongyi
# =============================================================================

class DashScopeProvider(BaseLLMProvider):
    """
    é˜¿é‡Œäº‘DashScope API (Qwen, DeepSeekç­‰) - åŸºäº LangChain ChatTongyi

    ğŸ”¥ æ”¯æŒ Function Calling (é€šè¿‡ tools å‚æ•°)
    """

    def _create_client(self):
        try:
            from langchain_community.chat_models import ChatTongyi
            api_key = self.config.api_key or os.getenv("DASHSCOPE_API_KEY")
            if not api_key:
                raise ValueError("ç¼ºå°‘DASHSCOPE_API_KEY")

            return ChatTongyi(
                model=self.config.model or "qwen-plus",
                temperature=self.config.temperature,
                dashscope_api_key=api_key,
                max_tokens=self.config.max_tokens,
            )
        except ImportError:
            raise ImportError("è¯·å®‰è£…langchain-community: pip install langchain-community")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage

        # ğŸ”¥ æ£€æŸ¥æ˜¯å¦æœ‰ tool æ¶ˆæ¯ï¼Œå¦‚æœæœ‰åˆ™ä½¿ç”¨åŸç”Ÿ DashScope API
        has_tool_messages = any(msg.get("role") == "tool" for msg in messages)
        tools = kwargs.get("tools")

        if has_tool_messages or (tools and self._has_tool_calls_in_messages(messages)):
            # ä½¿ç”¨åŸç”Ÿ DashScope API å¤„ç†å¤šè½®å·¥å…·è°ƒç”¨
            # ä» kwargs ä¸­ç§»é™¤ tools é¿å…é‡å¤ä¼ å‚
            kwargs_copy = {k: v for k, v in kwargs.items() if k != "tools"}
            return self._chat_with_native_api(messages, tools, **kwargs_copy)

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ (æ— å·¥å…·è°ƒç”¨æ—¶ä½¿ç”¨ LangChain)
        lc_messages = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "system":
                lc_messages.append(SystemMessage(content=content))
            elif role == "assistant":
                lc_messages.append(AIMessage(content=content))
            else:
                lc_messages.append(HumanMessage(content=content))

        # ğŸ”¥ å¦‚æœæä¾›äº†å·¥å…·å®šä¹‰ï¼Œç»‘å®šå·¥å…·
        client = self.client
        if tools:
            # è½¬æ¢ä¸º LangChain å·¥å…·æ ¼å¼
            lc_tools = self._convert_tools_to_langchain(tools)
            if lc_tools:
                client = self.client.bind_tools(lc_tools)

        # è°ƒç”¨ LangChain ChatTongyi
        response = client.invoke(lc_messages)

        # æå– token ä½¿ç”¨é‡
        usage = {}
        if hasattr(response, 'response_metadata'):
            meta = response.response_metadata
            if 'token_usage' in meta:
                token_usage = meta['token_usage']
                usage = {
                    "prompt_tokens": token_usage.get("input_tokens", 0),
                    "completion_tokens": token_usage.get("output_tokens", 0),
                    "total_tokens": token_usage.get("total_tokens", 0)
                }

        # ğŸ”¥ è§£æå·¥å…·è°ƒç”¨
        tool_calls = []
        finish_reason = "stop"
        if hasattr(response, 'tool_calls') and response.tool_calls:
            finish_reason = "tool_calls"
            for tc in response.tool_calls:
                tool_calls.append(ToolCall(
                    id=tc.get("id", f"call_{len(tool_calls)}"),
                    name=tc.get("name", ""),
                    arguments=tc.get("args", {})
                ))

        return LLMResponse(
            content=response.content or "",
            model=self.config.model,
            usage=usage,
            raw_response=response,
            tool_calls=tool_calls,
            finish_reason=finish_reason
        )

    def _has_tool_calls_in_messages(self, messages: List[Dict]) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦æœ‰ tool_calls"""
        return any(msg.get("tool_calls") for msg in messages)

    def _chat_with_native_api(self, messages: List[Dict], tools: List[Dict], **kwargs) -> LLMResponse:
        """
        ğŸ”¥ ä½¿ç”¨åŸç”Ÿ DashScope API å¤„ç†å¤šè½®å·¥å…·è°ƒç”¨

        LangChain å¯¹ tool messages çš„å¤„ç†æœ‰é—®é¢˜ï¼Œç›´æ¥ä½¿ç”¨ DashScope SDK
        """
        try:
            from dashscope import Generation
        except ImportError:
            raise ImportError("è¯·å®‰è£…dashscope: pip install dashscope")

        api_key = self.config.api_key or os.getenv("DASHSCOPE_API_KEY")

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ä¸º DashScope åŸç”Ÿæ ¼å¼
        ds_messages = []
        for msg in messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")

            if role == "assistant" and msg.get("tool_calls"):
                # å¸¦å·¥å…·è°ƒç”¨çš„ assistant æ¶ˆæ¯
                ds_msg = {
                    "role": "assistant",
                    "content": content or "",
                    "tool_calls": [
                        {
                            "id": tc.get("id", f"call_{i}"),
                            "type": "function",
                            "function": {
                                "name": tc.get("name", ""),
                                "arguments": json.dumps(tc.get("args", tc.get("arguments", {})), ensure_ascii=False)
                            }
                        }
                        for i, tc in enumerate(msg["tool_calls"])
                    ]
                }
                ds_messages.append(ds_msg)
            elif role == "tool":
                # å·¥å…·è¿”å›ç»“æœ
                ds_messages.append({
                    "role": "tool",
                    "content": content,
                    "tool_call_id": msg.get("tool_call_id", "")
                })
            else:
                ds_messages.append({"role": role, "content": content})

        # è½¬æ¢å·¥å…·æ ¼å¼
        ds_tools = None
        if tools:
            ds_tools = []
            for tool in tools:
                if tool.get("type") == "function":
                    ds_tools.append(tool)
                elif "name" in tool:
                    ds_tools.append({
                        "type": "function",
                        "function": {
                            "name": tool.get("name", ""),
                            "description": tool.get("description", ""),
                            "parameters": tool.get("parameters", {})
                        }
                    })

        # è°ƒç”¨ DashScope API
        response = Generation.call(
            api_key=api_key,
            model=self.config.model or "qwen-plus",
            messages=ds_messages,
            tools=ds_tools,
            result_format="message",
            temperature=kwargs.get("temperature", self.config.temperature),
            max_tokens=kwargs.get("max_tokens", self.config.max_tokens),
        )

        # è§£æå“åº”
        if response.status_code != 200:
            raise ValueError(f"DashScope API é”™è¯¯: {response.code} - {response.message}")

        output = response.output
        message = output.choices[0].message

        # è§£æå·¥å…·è°ƒç”¨
        tool_calls = []
        finish_reason = output.choices[0].finish_reason

        # å®‰å…¨è·å– tool_calls (DashScope å“åº”å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ)
        try:
            msg_tool_calls = message.tool_calls if hasattr(message, 'tool_calls') else None
        except (KeyError, AttributeError):
            msg_tool_calls = None

        if msg_tool_calls:
            for tc in msg_tool_calls:
                try:
                    # tc å¯èƒ½æ˜¯å¯¹è±¡æˆ–å­—å…¸
                    if isinstance(tc, dict):
                        func = tc.get("function", {})
                        tc_id = tc.get("id", f"call_{len(tool_calls)}")
                        func_name = func.get("name", "") if isinstance(func, dict) else ""
                        func_args = func.get("arguments", "{}") if isinstance(func, dict) else "{}"
                    else:
                        tc_id = tc.id if hasattr(tc, 'id') else f"call_{len(tool_calls)}"
                        func_name = tc.function.name
                        func_args = tc.function.arguments

                    args = json.loads(func_args) if func_args else {}
                except (json.JSONDecodeError, AttributeError):
                    args = {}

                tool_calls.append(ToolCall(
                    id=tc_id,
                    name=func_name,
                    arguments=args
                ))

        # æå– usage
        usage = {}
        if hasattr(response, 'usage'):
            usage = {
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
                "total_tokens": response.usage.total_tokens
            }

        return LLMResponse(
            content=message.content or "",
            model=self.config.model,
            usage=usage,
            raw_response=response,
            tool_calls=tool_calls,
            finish_reason=finish_reason
        )

    def _convert_tools_to_langchain(self, tools: List[Dict]) -> List[Dict]:
        """
        å°† OpenAI æ ¼å¼çš„ tools è½¬æ¢ä¸º LangChain æ ¼å¼

        OpenAI æ ¼å¼:
        [{"type": "function", "function": {"name": "...", "description": "...", "parameters": {...}}}]

        LangChain æ ¼å¼:
        [{"name": "...", "description": "...", "parameters": {...}}]
        """
        lc_tools = []
        for tool in tools:
            if tool.get("type") == "function":
                func = tool.get("function", {})
                lc_tools.append({
                    "name": func.get("name", ""),
                    "description": func.get("description", ""),
                    "parameters": func.get("parameters", {})
                })
            elif "name" in tool:
                # å·²ç»æ˜¯ç®€åŒ–æ ¼å¼
                lc_tools.append(tool)
        return lc_tools

    def is_available(self) -> bool:
        api_key = self.config.api_key or os.getenv("DASHSCOPE_API_KEY")
        return bool(api_key)


# =============================================================================
# Ollama Provider (æœ¬åœ°æ¨¡å‹)
# =============================================================================

class OllamaProvider(BaseLLMProvider):
    """Ollamaæœ¬åœ°æ¨¡å‹"""

    def _create_client(self):
        try:
            import ollama
            return ollama
        except ImportError:
            raise ImportError("è¯·å®‰è£…ollama: pip install ollama")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        response = self.client.chat(
            model=self.config.model or "llama3.3",
            messages=messages,
            options={
                "temperature": kwargs.get("temperature", self.config.temperature),
                "num_predict": kwargs.get("max_tokens", self.config.max_tokens),
            }
        )
        return LLMResponse(
            content=response["message"]["content"],
            model=self.config.model,
            usage={
                "prompt_tokens": response.get("prompt_eval_count", 0),
                "completion_tokens": response.get("eval_count", 0),
                "total_tokens": response.get("prompt_eval_count", 0) + response.get("eval_count", 0)
            },
            raw_response=response
        )

    def is_available(self) -> bool:
        try:
            import ollama
            ollama.list()
            return True
        except:
            return False


# =============================================================================
# OpenAI Compatible Provider (é€šç”¨)
# =============================================================================

class OpenAICompatibleProvider(BaseLLMProvider):
    """å…¼å®¹OpenAI APIçš„é€šç”¨Provider"""

    def _create_client(self):
        try:
            from openai import OpenAI
            return OpenAI(
                api_key=self.config.api_key,
                base_url=self.config.base_url,
                timeout=self.config.timeout
            )
        except ImportError:
            raise ImportError("è¯·å®‰è£…openai: pip install openai")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        response = self.client.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=kwargs.get("temperature", self.config.temperature),
            max_tokens=kwargs.get("max_tokens", self.config.max_tokens),
            **self.config.extra_params
        )
        return LLMResponse(
            content=response.choices[0].message.content,
            model=response.model,
            usage={
                "prompt_tokens": getattr(response.usage, "prompt_tokens", 0),
                "completion_tokens": getattr(response.usage, "completion_tokens", 0),
                "total_tokens": getattr(response.usage, "total_tokens", 0)
            },
            raw_response=response
        )

    def is_available(self) -> bool:
        return bool(self.config.api_key and self.config.base_url)


# =============================================================================
# Provider Factory
# =============================================================================

class LLMProviderFactory:
    """LLM Providerå·¥å‚"""

    _providers = {
        ProviderType.OPENAI: OpenAIProvider,
        ProviderType.ANTHROPIC: AnthropicProvider,
        ProviderType.GOOGLE: GoogleProvider,
        ProviderType.DEEPSEEK: DeepSeekProvider,
        ProviderType.ZHIPU: ZhipuProvider,
        ProviderType.DASHSCOPE: DashScopeProvider,
        ProviderType.OLLAMA: OllamaProvider,
        ProviderType.OPENAI_COMPATIBLE: OpenAICompatibleProvider,
    }

    @classmethod
    def create(cls, config: LLMConfig) -> BaseLLMProvider:
        """åˆ›å»ºProviderå®ä¾‹"""
        provider_class = cls._providers.get(config.provider)
        if not provider_class:
            raise ValueError(f"ä¸æ”¯æŒçš„Provider: {config.provider}")
        return provider_class(config)

    @classmethod
    def create_from_env(
        cls,
        provider: Union[str, ProviderType],
        model: Optional[str] = None,
        **kwargs
    ) -> BaseLLMProvider:
        """ä»ç¯å¢ƒå˜é‡åˆ›å»ºProvider"""
        if isinstance(provider, str):
            provider = ProviderType(provider.lower())

        # é»˜è®¤æ¨¡å‹
        default_models = {
            ProviderType.OPENAI: "gpt-4o",
            ProviderType.ANTHROPIC: "claude-sonnet-4-5",
            ProviderType.GOOGLE: "gemini-3-flash",
            ProviderType.DEEPSEEK: "deepseek-chat",
            ProviderType.ZHIPU: "glm-4.7",
            ProviderType.DASHSCOPE: "qwen-max",
            ProviderType.OLLAMA: "llama3.3",
        }

        config = LLMConfig(
            provider=provider,
            model=model or default_models.get(provider, ""),
            **kwargs
        )
        return cls.create(config)

    @classmethod
    def get_available_providers(cls) -> List[ProviderType]:
        """è·å–å½“å‰å¯ç”¨çš„Provideråˆ—è¡¨"""
        available = []
        for provider_type, provider_class in cls._providers.items():
            try:
                config = LLMConfig(provider=provider_type, model="test")
                instance = provider_class(config)
                if instance.is_available():
                    available.append(provider_type)
            except:
                pass
        return available


# =============================================================================
# Multi-Model Manager
# =============================================================================

@dataclass
class ModelAssignment:
    """Agentçš„æ¨¡å‹åˆ†é…"""
    agent_role: str
    provider: ProviderType
    model: str
    fallback_provider: Optional[ProviderType] = None
    fallback_model: Optional[str] = None


class MultiModelManager:
    """
    å¤šæ¨¡å‹ç®¡ç†å™¨

    ä¸ºä¸åŒAgentåˆ†é…ä¸åŒçš„æ¨¡å‹ï¼Œæ”¯æŒfallbackæœºåˆ¶ã€‚
    """

    def __init__(self):
        self._providers: Dict[str, BaseLLMProvider] = {}
        self._assignments: Dict[str, ModelAssignment] = {}

    def assign(
        self,
        agent_role: str,
        provider: Union[str, ProviderType],
        model: str,
        fallback_provider: Optional[Union[str, ProviderType]] = None,
        fallback_model: Optional[str] = None
    ):
        """ä¸ºAgentåˆ†é…æ¨¡å‹"""
        if isinstance(provider, str):
            provider = ProviderType(provider.lower())
        if isinstance(fallback_provider, str):
            fallback_provider = ProviderType(fallback_provider.lower())

        self._assignments[agent_role] = ModelAssignment(
            agent_role=agent_role,
            provider=provider,
            model=model,
            fallback_provider=fallback_provider,
            fallback_model=fallback_model
        )

    def get_provider(self, agent_role: str) -> BaseLLMProvider:
        """è·å–Agentå¯¹åº”çš„Provider"""
        assignment = self._assignments.get(agent_role)
        if not assignment:
            raise ValueError(f"æœªæ‰¾åˆ°Agent '{agent_role}' çš„æ¨¡å‹åˆ†é…")

        cache_key = f"{assignment.provider.value}:{assignment.model}"

        if cache_key not in self._providers:
            config = LLMConfig(
                provider=assignment.provider,
                model=assignment.model
            )
            provider = LLMProviderFactory.create(config)

            # æ£€æŸ¥å¯ç”¨æ€§ï¼Œå°è¯•fallback
            if not provider.is_available() and assignment.fallback_provider:
                print(f"[MultiModel] {assignment.provider.value} ä¸å¯ç”¨ï¼Œå°è¯• fallback...")
                fallback_config = LLMConfig(
                    provider=assignment.fallback_provider,
                    model=assignment.fallback_model or ""
                )
                provider = LLMProviderFactory.create(fallback_config)
                cache_key = f"{assignment.fallback_provider.value}:{assignment.fallback_model}"

            self._providers[cache_key] = provider

        return self._providers[cache_key]

    def configure_default(self):
        """é…ç½®é»˜è®¤çš„æ¨¡å‹åˆ†é…"""
        # æ ¹æ®å¯ç”¨çš„APIè‡ªåŠ¨åˆ†é…
        available = LLMProviderFactory.get_available_providers()

        if ProviderType.ANTHROPIC in available:
            # Claudeä½œä¸ºExpert (æœ€å¼ºæ¨ç†èƒ½åŠ›)
            self.assign("expert", ProviderType.ANTHROPIC, "claude-sonnet-4-20250514")
        elif ProviderType.DEEPSEEK in available:
            self.assign("expert", ProviderType.DEEPSEEK, "deepseek-chat")
        elif ProviderType.DASHSCOPE in available:
            self.assign("expert", ProviderType.DASHSCOPE, "qwen-max")

        if ProviderType.DEEPSEEK in available:
            # DeepSeekä½œä¸ºAuditor (æ€§ä»·æ¯”é«˜)
            self.assign("auditor", ProviderType.DEEPSEEK, "deepseek-chat")
        elif ProviderType.DASHSCOPE in available:
            self.assign("auditor", ProviderType.DASHSCOPE, "deepseek-v3.2")

        if ProviderType.DASHSCOPE in available:
            # Qwenä½œä¸ºAnalystå’ŒManager
            self.assign("analyst", ProviderType.DASHSCOPE, "qwen-max")
            self.assign("manager", ProviderType.DASHSCOPE, "qwen-max")
        elif ProviderType.OPENAI in available:
            self.assign("analyst", ProviderType.OPENAI, "gpt-4o")
            self.assign("manager", ProviderType.OPENAI, "gpt-4o")


# å…¨å±€å®ä¾‹
_model_manager: Optional[MultiModelManager] = None


def get_model_manager() -> MultiModelManager:
    """è·å–å…¨å±€MultiModelManagerå®ä¾‹"""
    global _model_manager
    if _model_manager is None:
        _model_manager = MultiModelManager()
        _model_manager.configure_default()
    return _model_manager
