"""
BaseAgent - AgentåŸºç±»

æ‰€æœ‰Agentçš„åŸºç±»ï¼Œå®šä¹‰é€šç”¨æ¥å£å’ŒLLMè°ƒç”¨é€»è¾‘ã€‚

æ”¯æŒå¤šæ¨¡å‹é…ç½®:
- é€šè¿‡AgentConfigæŒ‡å®šproviderå’Œmodel
- æ”¯æŒOpenAI, Anthropic, Google, DeepSeek, ZhipuAI, DashScope, Ollama
- æ”¯æŒfallbackæœºåˆ¶
"""

import asyncio
import json
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union
from enum import Enum

from src.utils.json_parser import robust_parse_json

# å¯¼å…¥å·¥å…·ç±»å‹
from typing import Callable

# å¯¼å…¥å¤šæ¨¡å‹ç³»ç»Ÿ
from src.llm_providers import (
    BaseLLMProvider,
    LLMProviderFactory,
    LLMConfig,
    ProviderType
)


class AgentRole(Enum):
    """Agentè§’è‰²æšä¸¾"""
    MANAGER = "manager"
    ANALYST = "analyst"
    AUDITOR = "auditor"
    EXPERT = "expert"


@dataclass
class AgentMessage:
    """Agenté—´é€šä¿¡æ¶ˆæ¯"""
    from_agent: AgentRole
    to_agent: AgentRole
    message_type: str  # "request" | "response" | "broadcast"
    content: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentConfig:
    """
    Agenté…ç½®

    æ”¯æŒä¸¤ç§æ¨¡å¼:
    1. ä¼ ç»Ÿæ¨¡å¼ (model_name): ä½¿ç”¨DashScope
    2. å¤šæ¨¡å‹æ¨¡å¼ (provider + model): ä½¿ç”¨æŒ‡å®šçš„Provider
    """
    # ä¼ ç»Ÿé…ç½® (å‘åå…¼å®¹)
    model_name: str = "qwen-max"

    # å¤šæ¨¡å‹é…ç½®
    provider: Optional[str] = None  # "openai", "anthropic", "deepseek", etc.
    model: Optional[str] = None     # å…·ä½“æ¨¡å‹å

    # é€šç”¨é…ç½®
    temperature: float = 0.1
    max_tokens: int = 4096
    max_retries: int = 3
    timeout: int = 120

    # Fallbacké…ç½®
    fallback_provider: Optional[str] = None
    fallback_model: Optional[str] = None

    # APIé…ç½® (å¯é€‰ï¼Œå¦åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–)
    api_key: Optional[str] = None
    base_url: Optional[str] = None


class BaseAgent(ABC):
    """
    AgentåŸºç±»

    æ‰€æœ‰Agentç»§æ‰¿æ­¤ç±»ï¼Œå®ç°ç‰¹å®šçš„åˆ†æé€»è¾‘ã€‚

    å¤šæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹:
    ```python
    # ä½¿ç”¨Claude
    config = AgentConfig(provider="anthropic", model="claude-sonnet-4-20250514")
    agent = MyAgent(config=config)

    # ä½¿ç”¨DeepSeek
    config = AgentConfig(provider="deepseek", model="deepseek-chat")
    agent = MyAgent(config=config)

    # ä½¿ç”¨GLM-4
    config = AgentConfig(provider="zhipu", model="glm-4-plus")
    agent = MyAgent(config=config)
    ```
    """

    def __init__(
        self,
        role: AgentRole,
        role_prompt: str,
        config: Optional[AgentConfig] = None
    ):
        self.role = role
        self.role_prompt = role_prompt
        self.config = config or AgentConfig()

        # åˆå§‹åŒ–LLM
        self._llm_provider: Optional[BaseLLMProvider] = None
        self._init_llm()

        # ğŸ”¥ LLM è°ƒç”¨é” - é˜²æ­¢åŒä¸€å®ä¾‹å¹¶å‘è°ƒç”¨ provider
        self._llm_lock = asyncio.Lock()

        # å¯¹è¯å†å²
        self.conversation_history: List[Dict[str, str]] = []

        # AgentçŠ¶æ€
        self.state: Dict[str, Any] = {}

        # ğŸ”¥ å·¥å…·ç®± (å¯é€‰ï¼Œç”¨äºè‡ªä¸»æ£€ç´¢)
        self.toolkit: Optional[Any] = None  # AgentToolkit å®ä¾‹

        # ğŸ”¥ v2.5.8: Token ä½¿ç”¨é‡ç»Ÿè®¡
        self._token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "call_count": 0
        }

    def set_toolkit(self, toolkit: Any):
        """
        è®¾ç½®å·¥å…·ç®±ï¼Œè®© Agent èƒ½å¤Ÿè‡ªä¸»è°ƒç”¨å·¥å…·æ£€ç´¢ä¿¡æ¯

        Args:
            toolkit: AgentToolkit å®ä¾‹
        """
        self.toolkit = toolkit

    def retrieve_context_for_finding(
        self,
        finding: Dict[str, Any],
        include_callers: bool = True,
        include_callees: bool = True,
        caller_depth: int = 2,
        callee_depth: int = 2
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ æ ¹æ® finding çš„ location è‡ªåŠ¨æ£€ç´¢ç›¸å…³ä»£ç ä¸Šä¸‹æ–‡

        æ›¿ä»£ç›´æ¥ä¼ å…¥æ•´ä¸ªæºä»£ç ï¼Œè®© Agent åªè·å–ä¸æ¼æ´ç›¸å…³çš„ä»£ç ç‰‡æ®µã€‚

        Args:
            finding: æ¼æ´å‘ç°ï¼Œéœ€åŒ…å« location: {module, function}
            include_callers: æ˜¯å¦åŒ…å«è°ƒç”¨è€…
            include_callees: æ˜¯å¦åŒ…å«è¢«è°ƒç”¨è€…
            caller_depth: è°ƒç”¨è€…æ£€ç´¢æ·±åº¦
            callee_depth: è¢«è°ƒç”¨è€…æ£€ç´¢æ·±åº¦

        Returns:
            {
                "target_function": {...},   # ç›®æ ‡å‡½æ•°ä»£ç 
                "callers": [...],           # è°ƒç”¨è€…åˆ—è¡¨
                "callees": [...],           # è¢«è°ƒç”¨è€…åˆ—è¡¨
                "types": [...],             # ç›¸å…³ç±»å‹å®šä¹‰
                "context_summary": "..."    # æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡æ‘˜è¦
            }
        """
        if not self.toolkit:
            return {"error": "No toolkit available", "context_summary": ""}

        location = finding.get("location", {})
        module = location.get("module", "")
        function = location.get("function", "")

        if not module or not function:
            # å°è¯•ä» title æˆ– description æå–
            title = finding.get("title", "")
            desc = finding.get("description", "")
            # ç®€å•æå–ï¼šæ‰¾ module::function æ¨¡å¼
            import re
            match = re.search(r'(\w+)::(\w+)', f"{title} {desc}")
            if match:
                module, function = match.groups()

        if not function:
            return {"error": "Cannot determine function location", "context_summary": ""}

        result = {
            "target_function": None,
            "callers": [],
            "callees": [],
            "types": [],
            "context_summary": ""
        }

        context_parts = []
        caller_tag = self.role.value  # ä½¿ç”¨ agent è§’è‰²ä½œä¸ºè°ƒç”¨è€…æ ‡è¯†

        # 1. è·å–ç›®æ ‡å‡½æ•°ä»£ç 
        func_result = self.toolkit.call_tool("get_function_code", {
            "module": module,
            "function": function
        }, caller=caller_tag)
        if func_result.success:
            result["target_function"] = func_result.data
            body = func_result.data.get("body", "")
            sig = func_result.data.get("signature", "")
            context_parts.append(f"## ç›®æ ‡å‡½æ•°: {module}::{function}\n```move\n{body}\n```")

            # ä»å‡½æ•°ä½“æå–å¯èƒ½çš„ç±»å‹å
            type_matches = re.findall(r'(\w+(?:Pool|Vault|Position|Config|Cap|Info|State))', body)
            for type_name in set(type_matches):
                type_result = self.toolkit.call_tool("get_type_definition", {"type_name": type_name}, caller=caller_tag)
                if type_result.success:
                    result["types"].append(type_result.data)

        # 2. è·å–è°ƒç”¨è€…
        if include_callers:
            callers_result = self.toolkit.call_tool("get_callers", {
                "module": module,
                "function": function,
                "depth": caller_depth
            }, caller=caller_tag)
            if callers_result.success:
                callers = callers_result.data.get("callers", [])
                result["callers"] = callers
                if callers:
                    caller_names = [c.get("id", c.get("name", "?")) for c in callers[:5]]
                    context_parts.append(f"## è°ƒç”¨è€… ({len(callers)} ä¸ª)\n" + "\n".join(f"- {n}" for n in caller_names))

        # 3. è·å–è¢«è°ƒç”¨è€…
        if include_callees:
            callees_result = self.toolkit.call_tool("get_callees", {
                "module": module,
                "function": function,
                "depth": callee_depth
            }, caller=caller_tag)
            if callees_result.success:
                callees = callees_result.data.get("callees", [])
                result["callees"] = callees
                if callees:
                    callee_names = [c.get("id", c.get("name", "?")) for c in callees[:5]]
                    context_parts.append(f"## è¢«è°ƒç”¨è€… ({len(callees)} ä¸ª)\n" + "\n".join(f"- {n}" for n in callee_names))

                    # è·å–å…³é”®è¢«è°ƒç”¨è€…çš„ä»£ç 
                    for callee in callees[:3]:
                        callee_id = callee.get("id", "")
                        if "::" in callee_id:
                            parts = callee_id.split("::")
                            callee_func = parts[-1]
                            callee_mod = "::".join(parts[:-1])
                            callee_code = self.toolkit.call_tool("get_function_code", {
                                "module": callee_mod,
                                "function": callee_func
                            }, caller=caller_tag)
                            if callee_code.success:
                                body = callee_code.data.get("body", "")[:500]
                                context_parts.append(f"### {callee_id}\n```move\n{body}\n```")

        # 4. è·å–ç±»å‹å®šä¹‰
        if result["types"]:
            types_str = "\n".join([
                f"### {t.get('name', '?')}\n```move\n{t.get('body', '')}\n```"
                for t in result["types"][:3]
            ])
            context_parts.append(f"## ç›¸å…³ç±»å‹å®šä¹‰\n{types_str}")

        result["context_summary"] = "\n\n".join(context_parts)
        return result

    def _init_llm(self):
        """åˆå§‹åŒ–LLM Provider (ä½¿ç”¨ config ä¸­çš„é…ç½®)"""
        provider_type = ProviderType(self.config.provider.lower())
        llm_config = LLMConfig(
            provider=provider_type,
            model=self.config.model or self.config.model_name,
            api_key=self.config.api_key,
            base_url=self.config.base_url,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            timeout=self.config.timeout
        )
        self._llm_provider = LLMProviderFactory.create(llm_config)

    def _track_token_usage(self, usage: Dict[str, int]):
        """ğŸ”¥ v2.5.8: ç´¯åŠ  token ä½¿ç”¨é‡"""
        if usage:
            self._token_usage["prompt_tokens"] += usage.get("prompt_tokens", 0)
            self._token_usage["completion_tokens"] += usage.get("completion_tokens", 0)
            self._token_usage["total_tokens"] += usage.get("total_tokens", 0)
            self._token_usage["call_count"] += 1

    def get_token_usage(self) -> Dict[str, int]:
        """ğŸ”¥ v2.5.8: è·å– token ä½¿ç”¨é‡ç»Ÿè®¡"""
        return self._token_usage.copy()

    def reset_token_usage(self):
        """ğŸ”¥ v2.5.8: é‡ç½® token ä½¿ç”¨é‡ç»Ÿè®¡"""
        self._token_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "call_count": 0
        }

    @abstractmethod
    async def process(self, message: AgentMessage) -> AgentMessage:
        """
        å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯

        Args:
            message: è¾“å…¥æ¶ˆæ¯

        Returns:
            å“åº”æ¶ˆæ¯
        """
        pass

    async def call_llm(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        json_mode: bool = False,
        stateless: bool = False
    ) -> str:
        """
        è°ƒç”¨LLM

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º (é»˜è®¤ä½¿ç”¨è§’è‰²æç¤º)
            json_mode: æ˜¯å¦è¦æ±‚JSONè¾“å‡º
            stateless: æ— çŠ¶æ€æ¨¡å¼ (ä¸ä½¿ç”¨/ä¸ä¿å­˜å¯¹è¯å†å²ï¼Œé€‚åˆå¹¶è¡Œè°ƒç”¨)

        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        system = system_prompt or self.role_prompt

        if json_mode:
            system += "\n\nè¯·ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœã€‚"

        # ğŸ”¥ stateless æ¨¡å¼ä¸ä½¿ç”¨å¯¹è¯å†å² (ç”¨äºå¹¶è¡Œè°ƒç”¨)
        if stateless:
            messages = [
                {"role": "system", "content": system},
                {"role": "user", "content": prompt}
            ]
        else:
            messages = [
                {"role": "system", "content": system},
                *self.conversation_history,
                {"role": "user", "content": prompt}
            ]

        # å¸¦é‡è¯•çš„ LLM è°ƒç”¨ (å¤„ç† 429 rate limit)
        # ğŸ”¥ å¢å¼ºé‡è¯•: æ›´å¤šæ¬¡æ•° + æ›´é•¿é€€é¿ + éšæœºæŠ–åŠ¨
        import random
        max_retries = max(self.config.max_retries, 5)  # è‡³å°‘5æ¬¡é‡è¯•
        base_delay = 3.0  # åŸºç¡€å»¶è¿Ÿ3ç§’
        max_delay = 30.0  # æœ€å¤§å»¶è¿Ÿ30ç§’

        for attempt in range(max_retries):
            try:
                # ğŸ”¥ stateless æ¨¡å¼ä¸éœ€è¦é”ï¼ˆä¸å†™ conversation_historyï¼Œå¯ä»¥çœŸæ­£å¹¶å‘ï¼‰
                # åªæœ‰ stateful æ¨¡å¼æ‰éœ€è¦é”ä¿æŠ¤ conversation_history
                async def _do_llm_call():
                    if self._llm_provider is not None:
                        response = await asyncio.to_thread(self._llm_provider.chat, messages)
                        # ğŸ”¥ v2.5.8: è¿½è¸ª token ä½¿ç”¨é‡
                        if hasattr(response, 'usage') and response.usage:
                            self._track_token_usage(response.usage)
                        return response.content
                    else:
                        response = await asyncio.to_thread(self.llm.invoke, messages)
                        return response.content if hasattr(response, 'content') else str(response)

                if stateless:
                    # ğŸ”¥ stateless æ¨¡å¼ï¼šæ— é”å¹¶å‘
                    result = await _do_llm_call()
                else:
                    # stateful æ¨¡å¼ï¼šåŠ é”ä¿æŠ¤ conversation_history
                    async with self._llm_lock:
                        result = await _do_llm_call()
                    self.conversation_history.append({"role": "user", "content": prompt})
                    self.conversation_history.append({"role": "assistant", "content": result})

                return result

            except Exception as e:
                error_str = str(e)
                # æ£€æŸ¥æ˜¯å¦æ˜¯ 429 rate limit é”™è¯¯
                if "429" in error_str or "rate" in error_str.lower() or "1302" in error_str:
                    if attempt < max_retries - 1:
                        # ğŸ”¥ æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ (é¿å…å¤šAgentåŒæ—¶é‡è¯•)
                        delay = min(base_delay * (2 ** attempt), max_delay)
                        jitter = random.uniform(0.5, 1.5)  # 0.5x ~ 1.5x éšæœºå› å­
                        actual_delay = delay * jitter
                        print(f"[{self.role.value}] â³ API é™æµï¼Œ{actual_delay:.1f}s åé‡è¯• ({attempt + 1}/{max_retries})...")
                        await asyncio.sleep(actual_delay)
                        continue

                print(f"[{self.role.value}] LLMè°ƒç”¨å¤±è´¥: {e}")
                raise

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise Exception(f"[{self.role.value}] API é™æµï¼Œ{max_retries}æ¬¡é‡è¯•å‡å¤±è´¥")

    async def call_llm_with_tools(
        self,
        prompt: str,
        tools: Optional[List[Dict]] = None,
        system_prompt: Optional[str] = None,
        max_tool_rounds: int = 5,  # ğŸ”¥ v2.5.14: é™ä½é»˜è®¤å€¼
        json_mode: bool = False
    ) -> str:
        """
        ğŸ”¥ å¸¦å·¥å…·è°ƒç”¨å¾ªç¯çš„ LLM è°ƒç”¨

        AI å¯ä»¥è‡ªä¸»å†³å®šè°ƒç”¨å“ªäº›å·¥å…·ï¼Œè·å–ç»“æœåç»§ç»­åˆ†æï¼Œ
        ç›´åˆ° AI è®¤ä¸ºåˆ†æå®Œæˆï¼ˆä¸å†è°ƒç”¨å·¥å…·ï¼‰ã€‚

        Args:
            prompt: ç”¨æˆ·æç¤º
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ (OpenAI æ ¼å¼)
            system_prompt: ç³»ç»Ÿæç¤º
            max_tool_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ¬¡
            json_mode: æœ€ç»ˆè¾“å‡ºæ˜¯å¦è¦æ±‚ JSON æ ¼å¼

        Returns:
            AI æœ€ç»ˆå“åº”æ–‡æœ¬
        """
        if not self.toolkit or not tools:
            # æ— å·¥å…·ï¼Œé€€åŒ–ä¸ºæ™®é€šè°ƒç”¨
            return await self.call_llm(prompt, system_prompt, json_mode, stateless=True)

        system = system_prompt or self.role_prompt
        if json_mode:
            system += "\n\nè¯·ä»¥JSONæ ¼å¼è¾“å‡ºæœ€ç»ˆç»“æœã€‚"

        # ğŸ”¥ è¦æ±‚ LLM åœ¨è°ƒç”¨å·¥å…·å‰å…ˆè¾“å‡ºåˆ†ææ€è·¯
        system += """

## ğŸš¨ å·¥å…·è°ƒç”¨è¯´æ˜
å¦‚æœä½ éœ€è¦è°ƒç”¨å·¥å…·è·å–æ›´å¤šä»£ç ï¼Œè¯·åœ¨è°ƒç”¨å·¥å…·çš„åŒæ—¶ï¼Œç”¨ä¸€ä¸¤å¥è¯è¯´æ˜ä½ çš„åˆ†ææ€è·¯å’Œä¸ºä»€ä¹ˆéœ€è¦è¿™äº›ä¿¡æ¯ã€‚
æ³¨æ„ï¼šç›´æ¥ä½¿ç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œä¸è¦æŠŠ "content:" æˆ– "tool_calls:" ä½œä¸ºæ–‡æœ¬è¾“å‡ºã€‚"""

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ]

        # ğŸ”¥ v2.5.5: å·¥å…·ç»“æœç¼“å­˜ (key -> result)
        # é‡å¤è°ƒç”¨æ—¶è¿”å›ç¼“å­˜ç»“æœè€Œä¸æ˜¯è·³è¿‡
        tool_result_cache: Dict[str, str] = {}

        # å·¥å…·è°ƒç”¨å¾ªç¯
        for round_num in range(max_tool_rounds):
            # è°ƒç”¨ LLM
            async with self._llm_lock:
                response = await asyncio.to_thread(
                    self._llm_provider.chat,
                    messages,
                    tools=tools
                )

            # ğŸ”¥ v2.5.8: è¿½è¸ª token ä½¿ç”¨é‡
            if hasattr(response, 'usage') and response.usage:
                self._track_token_usage(response.usage)

            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if response.tool_calls:
                # ğŸ”¥ æ˜¾ç¤º AI çš„åˆ†ææ€è€ƒè¿‡ç¨‹ (åœ¨è°ƒç”¨å·¥å…·ä¹‹å‰)
                if response.content and response.content.strip():
                    thinking = response.content.strip()
                    if len(thinking) > 200:
                        thinking_summary = thinking[:200] + "..."
                    else:
                        thinking_summary = thinking
                    print(f"    ğŸ’­ [{self.role.value}] AI åˆ†æ: {thinking_summary}")
                else:
                    # ğŸ”¥ å¦‚æœ content ä¸ºç©ºï¼Œæ ¹æ®å·¥å…·è°ƒç”¨æ¨æ–­æ„å›¾
                    intent_parts = []
                    for tc in response.tool_calls[:3]:
                        args = tc.arguments
                        if tc.name == "get_function_code":
                            intent_parts.append(f"æŸ¥çœ‹ {args.get('function', '?')}")
                        elif tc.name == "get_callers":
                            intent_parts.append(f"æŸ¥æ‰¾ {args.get('function', '?')} å…¥å£")
                        elif tc.name == "get_callees":
                            intent_parts.append(f"è¿½è¸ª {args.get('function', '?')} è°ƒç”¨é“¾")
                        elif tc.name == "get_type_definition":
                            intent_parts.append(f"æŸ¥çœ‹ {args.get('type_name', '?')} ç±»å‹")
                    if intent_parts:
                        print(f"    ğŸ’­ [{self.role.value}] æ„å›¾: {', '.join(intent_parts)}")

                print(f"    ğŸ”§ [{self.role.value}] Round {round_num + 1}: AI è¯·æ±‚è°ƒç”¨ {len(response.tool_calls)} ä¸ªå·¥å…·")

                # ğŸ”¥ v2.5.4: åˆ†ç¦»æ–°è°ƒç”¨å’Œç¼“å­˜å‘½ä¸­
                new_tool_calls = []
                cached_tool_calls = []
                for tc in response.tool_calls:
                    tool_key = f"{tc.name}:{json.dumps(tc.arguments, sort_keys=True)}"
                    if tool_key in tool_result_cache:
                        cached_tool_calls.append((tc, tool_key))
                    else:
                        new_tool_calls.append((tc, tool_key))

                if cached_tool_calls:
                    print(f"       ğŸ“Œ ç¼“å­˜å‘½ä¸­: {len(cached_tool_calls)} ä¸ª (è·³è¿‡é‡å¤æ‰§è¡Œ)")

                # å¦‚æœå…¨éƒ¨å‘½ä¸­ç¼“å­˜ï¼Œæ³¨å…¥ç¼“å­˜ç»“æœå¹¶ç»§ç»­
                if not new_tool_calls and cached_tool_calls:
                    # æ·»åŠ  AI æ¶ˆæ¯
                    messages.append({
                        "role": "assistant",
                        "content": response.content or "",
                        "tool_calls": [
                            {"id": tc.id, "name": tc.name, "args": tc.arguments}
                            for tc, _ in cached_tool_calls
                        ]
                    })
                    # æ³¨å…¥ç¼“å­˜ç»“æœ (v2.5.5: ç§»é™¤ [ç¼“å­˜] å‰ç¼€ï¼Œé¿å…å¹²æ‰° LLM è§£æ)
                    for tc, tool_key in cached_tool_calls:
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tc.id,
                            "content": tool_result_cache[tool_key]
                        })
                    print(f"    âš ï¸ [{self.role.value}] å…¨éƒ¨ç¼“å­˜å‘½ä¸­ï¼Œæ³¨å…¥å†å²ç»“æœç»§ç»­...")
                    continue

                # æ·»åŠ  AI æ¶ˆæ¯ï¼ˆåŒ…å«æ‰€æœ‰ tool_callsï¼‰
                all_tool_calls = new_tool_calls + cached_tool_calls
                messages.append({
                    "role": "assistant",
                    "content": response.content or "",
                    "tool_calls": [
                        {"id": tc.id, "name": tc.name, "args": tc.arguments}
                        for tc, _ in all_tool_calls
                    ]
                })

                # æ‰§è¡Œæ–°å·¥å…·è°ƒç”¨ + æ³¨å…¥ç¼“å­˜ç»“æœ
                for tc, tool_key in all_tool_calls:
                    if tool_key in tool_result_cache:
                        # ğŸ”¥ ç¼“å­˜å‘½ä¸­ï¼šç›´æ¥æ³¨å…¥å†å²ç»“æœ
                        tool_output = tool_result_cache[tool_key]
                        print(f"       ğŸ“Œ Cache: {tc.name} â†’ ä½¿ç”¨ç¼“å­˜ç»“æœ")
                    else:
                        # æ–°è°ƒç”¨ï¼šæ‰§è¡Œå¹¶ç¼“å­˜
                        print(f"       ğŸ”§ Tool: {tc.name}({tc.arguments})")
                        result = self.toolkit.call_tool(tc.name, tc.arguments, caller=self.role.value)

                        if result.success:
                            tool_output = json.dumps(result.data, ensure_ascii=False, default=str)
                            print(f"          â†’ æˆåŠŸ")
                        else:
                            tool_output = f"é”™è¯¯: {result.error}"
                            print(f"          â†’ å¤±è´¥: {result.error}")

                        # ğŸ”¥ ç¼“å­˜ç»“æœ
                        tool_result_cache[tool_key] = tool_output

                    # æ·»åŠ å·¥å…·ç»“æœæ¶ˆæ¯
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tc.id,
                        "content": tool_output
                    })
            else:
                # AI ä¸å†è°ƒç”¨å·¥å…·ï¼Œè¿”å›æœ€ç»ˆå“åº”
                print(f"    âœ… [{self.role.value}] åˆ†æå®Œæˆ (å…± {round_num + 1} è½®)")
                return response.content

        # è¾¾åˆ°æœ€å¤§è½®æ¬¡ - ğŸ”¥ å‘é€æœ€ç»ˆè¯·æ±‚è¦æ±‚è¾“å‡ºç»“æœ
        print(f"    âš ï¸ [{self.role.value}] è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è½®æ¬¡ ({max_tool_rounds})ï¼Œè¯·æ±‚æœ€ç»ˆè¾“å‡º...")

        # æ·»åŠ æœ€ç»ˆæç¤ºï¼Œå¼ºåˆ¶ LLM åœæ­¢å·¥å…·è°ƒç”¨å¹¶è¾“å‡ºç»“æœ
        messages.append({
            "role": "user",
            "content": "è¯·åœæ­¢å·¥å…·è°ƒç”¨ã€‚åŸºäºä½ å·²ç»æ”¶é›†åˆ°çš„æ‰€æœ‰ä»£ç ä¿¡æ¯ï¼Œç«‹å³è¾“å‡ºæœ€ç»ˆçš„åˆ†æç»“æœã€‚" +
                      ("\nè¯·ç¡®ä¿è¾“å‡ºç¬¦åˆ JSON æ ¼å¼ã€‚" if json_mode else "")
        })

        # æœ€åä¸€æ¬¡è°ƒç”¨ LLMï¼ˆä¸å¸¦ tools å‚æ•°ï¼Œå¼ºåˆ¶æ–‡æœ¬è¾“å‡ºï¼‰
        try:
            async with self._llm_lock:
                final_response = await asyncio.to_thread(
                    self._llm_provider.chat,
                    messages
                    # ä¸ä¼  toolsï¼Œå¼ºåˆ¶è¾“å‡º
                )
            # ğŸ”¥ v2.5.8: è¿½è¸ª token ä½¿ç”¨é‡
            if hasattr(final_response, 'usage') and final_response.usage:
                self._track_token_usage(final_response.usage)
            print(f"    âœ… [{self.role.value}] æœ€ç»ˆè¾“å‡ºè·å–æˆåŠŸ")
            return final_response.content
        except Exception as e:
            print(f"    âš ï¸ [{self.role.value}] æœ€ç»ˆè¾“å‡ºè¯·æ±‚å¤±è´¥: {e}")
            return response.content if response else ""

    async def verify_with_tools(
        self,
        finding: Dict[str, Any],
        verification_prompt: str,
        function_index: str = "",
        analysis_context: str = "",
        max_tool_rounds: int = 3  # ğŸ”¥ v2.5.14: å·²æœ‰é¢„æ„å»ºä¸Šä¸‹æ–‡ï¼Œ3è½®è¶³å¤Ÿ
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ é€šç”¨å·¥å…·è¾…åŠ©éªŒè¯æ–¹æ³• (æ‰€æœ‰ Agent å¯ç”¨)

        AI å¯ä»¥è‡ªä¸»è°ƒç”¨å·¥å…·æŸ¥çœ‹ä»£ç æ¥éªŒè¯æ¼æ´ã€‚

        Args:
            finding: æ¼æ´å‘ç° (æ¥è‡ª Phase 2)
            verification_prompt: éªŒè¯ä»»åŠ¡çš„å…·ä½“ prompt (å„ Agent è‡ªå®šä¹‰)
            function_index: å‡½æ•°ç´¢å¼• (è®© AI çŸ¥é“å¯ä»¥æŸ¥è¯¢å“ªäº›å‡½æ•°)
            analysis_context: Phase 0/1 çš„åˆ†æä¸Šä¸‹æ–‡
            max_tool_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•° (é»˜è®¤ 8)

        Returns:
            éªŒè¯ç»“æœ (JSON è§£æåçš„å­—å…¸)

        Usage:
            # åœ¨å„ Agent ä¸­ä½¿ç”¨
            result = await self.verify_with_tools(
                finding=finding,
                verification_prompt="è¯·ä»å®‰å…¨è§’åº¦éªŒè¯...",
                function_index=toolkit.get_function_index(),
                analysis_context=toolkit.get_analysis_context()
            )
        """
        if not self.toolkit:
            raise ValueError(f"[{self.role.value}] verify_with_tools éœ€è¦è®¾ç½® toolkit")

        # è·å–åˆå§‹ä»£ç ä¸Šä¸‹æ–‡
        code_context = finding.get('code_context', '')
        evidence = finding.get('evidence', finding.get('proof', ''))
        location = finding.get('location', {})

        # è·å–å¯ç”¨å·¥å…·
        tools = self.toolkit.get_security_tools()

        # ğŸ”¥ åˆ¤æ–­æ˜¯å¦æœ‰é¢„æ„å»ºä¸Šä¸‹æ–‡
        has_prebuilt = bool(code_context and len(code_context.strip()) > 100)

        # æ„å»ºå®Œæ•´ prompt
        full_prompt = f"""
## æ¼æ´ä¿¡æ¯
- ID: {finding.get('id')}
- æ ‡é¢˜: {finding.get('title')}
- ä¸¥é‡æ€§: {finding.get('severity')}
- ä½ç½®: {location}
- æè¿°: {finding.get('description')}
- æ¼æ´ä»£ç /è¯æ®: {evidence}

## ğŸ”¥ é¢„æ„å»ºçš„ä»£ç ä¸Šä¸‹æ–‡
```move
{code_context if code_context else 'æ— åˆå§‹ä»£ç ä¸Šä¸‹æ–‡ï¼Œè¯·ä½¿ç”¨å·¥å…·è·å–'}
```

## ğŸ“ å·¥å…·ä½¿ç”¨æŒ‡å—
{"ä¸Šé¢å·²ç»æä¾›äº†æ¼æ´å‡½æ•°åŠå…¶è°ƒç”¨é“¾çš„ä»£ç ï¼Œè¯·ä¼˜å…ˆåŸºäºè¿™äº›ä»£ç è¿›è¡ŒéªŒè¯ã€‚" if has_prebuilt else "è¯·ä½¿ç”¨å·¥å…·è·å–ä»£ç è¿›è¡ŒéªŒè¯ã€‚"}
- **åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µæ‰éœ€è¦è°ƒç”¨å·¥å…·**ï¼šéœ€è¦è·¨æ¨¡å—å‡½æ•°ã€ç±»å‹å®šä¹‰ã€æ›´æ·±è°ƒç”¨é“¾
- **æ•ˆç‡è¦æ±‚**ï¼šæ¯è½®æœ€å¤šè°ƒç”¨ 2 ä¸ªå·¥å…·ï¼Œé¿å…é‡å¤è°ƒç”¨

{f"## Phase 0/1 åˆ†æä¸Šä¸‹æ–‡{chr(10)}{analysis_context}" if analysis_context else ""}

{f"## å¯æŸ¥è¯¢çš„å‡½æ•°{chr(10)}{function_index}" if function_index else ""}

## å¯ç”¨å·¥å…· (æŒ‰éœ€ä½¿ç”¨)
- `get_function_code(module, function)`: è·å–æŒ‡å®šå‡½æ•°çš„å®Œæ•´å®ç°
- `get_callers(module, function)`: æŸ¥çœ‹è°è°ƒç”¨äº†è¯¥å‡½æ•°
- `get_callees(module, function)`: æŸ¥çœ‹è¯¥å‡½æ•°è°ƒç”¨äº†ä»€ä¹ˆ
- `get_type_definition(type_name)`: æŸ¥çœ‹ struct ç±»å‹å®šä¹‰
- `search_code(pattern)`: åœ¨ä»£ç ä¸­æœç´¢æ¨¡å¼

## éªŒè¯ä»»åŠ¡
{verification_prompt}
"""
        # ä½¿ç”¨å·¥å…·è°ƒç”¨å¾ªç¯
        response = await self.call_llm_with_tools(
            prompt=full_prompt,
            tools=tools,
            max_tool_rounds=max_tool_rounds,
            json_mode=True
        )
        return self.parse_json_response(response)

    async def verify_lightweight(
        self,
        finding: Dict[str, Any],
        verification_prompt: str,
        minimal_context: str = "",
        function_index: str = "",
        max_tool_rounds: int = 3
    ) -> Dict[str, Any]:
        """
        ğŸ”¥ è½»é‡çº§å­ Agent éªŒè¯ (v2.4.9)

        æ ¸å¿ƒä¼˜åŒ–ï¼š
        1. åˆ›å»ºç‹¬ç«‹çš„ LLM å®ä¾‹ï¼Œä¸å…±äº«ä¸» Agent çš„é”
        2. åªä½¿ç”¨æœ€å°ä¸Šä¸‹æ–‡ (~5K tokens)ï¼Œä¸ç´¯ç§¯å†å²
        3. æ”¯æŒå¹¶è¡Œæ‰§è¡Œå¤šä¸ªéªŒè¯ä»»åŠ¡

        ä¸ verify_with_tools çš„åŒºåˆ«ï¼š
        - verify_with_tools: ä½¿ç”¨ self._llm_providerï¼Œå—é”ä¿æŠ¤ï¼Œä¸Šä¸‹æ–‡ç´¯ç§¯
        - verify_lightweight: ç‹¬ç«‹ LLMï¼Œæ— é”ï¼Œæœ€å°ä¸Šä¸‹æ–‡ï¼Œæ¯æ¬¡è°ƒç”¨éš”ç¦»

        Args:
            finding: æ¼æ´ä¿¡æ¯ (åªæå–å…³é”®å­—æ®µ)
            verification_prompt: éªŒè¯ä»»åŠ¡æè¿°
            minimal_context: æœ€å°ä»£ç ä¸Šä¸‹æ–‡ (ç›®æ ‡å‡½æ•° + å…³é”®è°ƒç”¨)
            function_index: å‡½æ•°ç´¢å¼• (è®© AI çŸ¥é“å¯æŸ¥è¯¢å“ªäº›å‡½æ•°)
            max_tool_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°

        Returns:
            éªŒè¯ç»“æœ (JSON)
        """
        import random

        if not self.toolkit:
            raise ValueError(f"[{self.role.value}] verify_lightweight éœ€è¦è®¾ç½® toolkit")

        # ğŸ”¥ åˆ›å»ºç‹¬ç«‹çš„ LLM å®ä¾‹ (ä¸ä½¿ç”¨ self._llm_provider)
        provider_type = ProviderType(self.config.provider.lower())
        llm_config = LLMConfig(
            provider=provider_type,
            model=self.config.model or self.config.model_name,
            api_key=self.config.api_key,
            base_url=self.config.base_url,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
            timeout=self.config.timeout
        )
        sub_agent_llm = LLMProviderFactory.create(llm_config)

        # ğŸ”¥ ç²¾ç®€çš„ç³»ç»Ÿ prompt
        system_prompt = f"""ä½ æ˜¯ {self.role.value} éªŒè¯å­ç¨‹åºã€‚ä¸“æ³¨éªŒè¯å•ä¸ªæ¼æ´ã€‚

å·¥ä½œåŸåˆ™ï¼š
1. ç›´æ¥åˆ†ææä¾›çš„ä»£ç ï¼Œé«˜æ•ˆä½¿ç”¨å·¥å…·
2. æ¯è½®æœ€å¤šè°ƒç”¨ 2 ä¸ªå·¥å…·
3. æ”¶é›†è¶³å¤Ÿä¿¡æ¯åç«‹å³è¾“å‡º JSON ç»“æœ
4. ä¸è¦é‡å¤è·å–ç›¸åŒä¿¡æ¯

{verification_prompt}"""

        # ğŸ”¥ æœ€å°ç”¨æˆ· prompt (åªå«å¿…è¦ä¿¡æ¯)
        vuln_id = finding.get('id', finding.get('title', 'unknown'))[:50]
        severity = finding.get('severity', 'unknown')
        description = finding.get('description', '')[:500]
        location = finding.get('location', {})
        location_str = f"{location.get('module', '?')}::{location.get('function', '?')}" if isinstance(location, dict) else str(location)

        user_prompt = f"""## æ¼æ´
- ID: {vuln_id}
- ä¸¥é‡æ€§: {severity}
- ä½ç½®: {location_str}
- æè¿°: {description}

## ä»£ç ä¸Šä¸‹æ–‡
```move
{minimal_context[:6000] if minimal_context else "è¯·ä½¿ç”¨å·¥å…·è·å–"}
```

{f"## å¯æŸ¥è¯¢å‡½æ•°{chr(10)}{function_index[:1500]}" if function_index else ""}

è¯·éªŒè¯æ­¤æ¼æ´å¹¶è¾“å‡º JSON ç»“æœã€‚"""

        # ğŸ”¥ å…¨æ–°çš„æ¶ˆæ¯åˆ—è¡¨ (ä¸å¸¦å†å²)
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # è·å–å·¥å…·
        tools = self.toolkit.get_security_tools()

        # å·¥å…·è°ƒç”¨å»é‡
        called_tools: set = set()

        def get_tool_key(name: str, args: dict) -> str:
            return f"{name}:{json.dumps(args, sort_keys=True, ensure_ascii=False)}"

        # ğŸ”¥ è½»é‡çº§å·¥å…·è°ƒç”¨å¾ªç¯ (ä½¿ç”¨ç‹¬ç«‹ LLMï¼Œæ— é”)
        for round_num in range(max_tool_rounds):
            try:
                # ä¸éœ€è¦é”ï¼Œå› ä¸ºæ˜¯ç‹¬ç«‹çš„ LLM å®ä¾‹
                # ğŸ”¥ ä¿®å¤: tools å¿…é¡»ä½œä¸ºå…³é”®å­—å‚æ•°ä¼ é€’
                response = await asyncio.to_thread(
                    lambda: sub_agent_llm.chat(messages, tools=tools)
                )
                # ğŸ”¥ v2.5.8: è¿½è¸ªå­ Agent token ä½¿ç”¨é‡
                if hasattr(response, 'usage') and response.usage:
                    self._track_token_usage(response.usage)
            except Exception as e:
                error_str = str(e)
                if "429" in error_str or "rate" in error_str.lower():
                    delay = 2.0 * (2 ** round_num) * random.uniform(0.5, 1.5)
                    print(f"      â³ [{self.role.value}] API é™æµï¼Œ{delay:.1f}s åé‡è¯•...")
                    await asyncio.sleep(delay)
                    continue
                return {"error": f"å­ Agent è°ƒç”¨å¤±è´¥: {str(e)[:100]}", "verification_result": "error"}

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if response.finish_reason != "tool_calls" or not response.tool_calls:
                return self.parse_json_response(response.content or "")

            # è¿‡æ»¤é‡å¤å·¥å…·è°ƒç”¨
            unique_calls = []
            for tc in response.tool_calls:
                tool_key = get_tool_key(tc.name, tc.arguments)
                if tool_key not in called_tools:
                    called_tools.add(tool_key)
                    unique_calls.append(tc)

            if not unique_calls:
                # å¼ºåˆ¶è¾“å‡º
                messages.append({"role": "user", "content": "è¯·ç«‹å³è¾“å‡º JSON ç»“æœã€‚"})
                try:
                    final_resp = await asyncio.to_thread(sub_agent_llm.chat, messages)
                    # ğŸ”¥ v2.5.8: è¿½è¸ªå­ Agent token ä½¿ç”¨é‡
                    if hasattr(final_resp, 'usage') and final_resp.usage:
                        self._track_token_usage(final_resp.usage)
                    return self.parse_json_response(final_resp.content or "")
                except:
                    break

            # è®°å½•å¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
            messages.append({
                "role": "assistant",
                "content": response.content or "",
                "tool_calls": [{"id": tc.id, "name": tc.name, "args": tc.arguments} for tc in unique_calls]
            })

            for tc in unique_calls:
                result = self.toolkit.call_tool(tc.name, tc.arguments, caller=f"Sub-{self.role.value}")
                tool_output = json.dumps(result.data, ensure_ascii=False)[:2000] if result.success else f"Error: {result.error}"
                messages.append({"role": "tool", "tool_call_id": tc.id, "content": tool_output})

        # æœ€å¤§è½®æ¬¡è€—å°½
        messages.append({"role": "user", "content": "è¯·ç«‹å³è¾“å‡º JSON ç»“æœã€‚"})
        try:
            final_resp = await asyncio.to_thread(sub_agent_llm.chat, messages)
            # ğŸ”¥ v2.5.8: è¿½è¸ªå­ Agent token ä½¿ç”¨é‡
            if hasattr(final_resp, 'usage') and final_resp.usage:
                self._track_token_usage(final_resp.usage)
            return self.parse_json_response(final_resp.content or "")
        except:
            return {"error": "å­ Agent è½®æ¬¡è€—å°½", "verification_result": "error"}

    def parse_json_response(self, response: str) -> Dict[str, Any]:
        """
        è§£æLLMçš„JSONå“åº”ï¼ˆä½¿ç”¨ json_parser å·¥å…·æ¨¡å—çš„ 9 ç§ä¿®å¤ç­–ç•¥ï¼‰

        Args:
            response: LLMå“åº”æ–‡æœ¬

        Returns:
            è§£æåçš„å­—å…¸
        """
        return robust_parse_json(response, verbose=True)

    def reset_conversation(self):
        """é‡ç½®å¯¹è¯å†å²"""
        self.conversation_history = []

    def get_state(self, key: str, default: Any = None) -> Any:
        """è·å–AgentçŠ¶æ€"""
        return self.state.get(key, default)

    def set_state(self, key: str, value: Any):
        """è®¾ç½®AgentçŠ¶æ€"""
        self.state[key] = value

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__} role={self.role.value}>"
