import argparse
import json
import os
import re
from pathlib import Path
from typing import Dict, List, Optional, Set

# å¯é€‰ä¾èµ–ï¼šdotenv (ç”¨äºå‘é‡åº“åŠŸèƒ½)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv ä¸æ˜¯å¿…éœ€çš„

try:
    from langchain_chroma import Chroma
    from langchain_core.documents import Document
    from langchain_community.embeddings import DashScopeEmbeddings
except ImportError:
    Chroma = None  # type: ignore
    Document = None  # type: ignore
    DashScopeEmbeddings = None  # type: ignore

BASE_DIR = Path(__file__).resolve().parents[2]
DEFAULT_INPUT_DIR = BASE_DIR / "data" / "knowledge_base" / "security"
DEFAULT_OUTPUT = BASE_DIR / "reports" / "datasets" / "security_patterns.jsonl"
DEFAULT_VECTOR_DIR = BASE_DIR / "data" / "vector_store" / "security_patterns"
DEFAULT_COLLECTION = "security_patterns"

SEVERITY_KEYWORDS = ["critical", "high", "medium", "low", "advisory"]

KEYWORD_TAGS = {
    "overflow": "overflow",
    "underflow": "overflow",
    "access control": "access_control",
    "permission": "access_control",
    "admin": "access_control",
    "oracle": "oracle",
    "price": "oracle",
    "time": "time_validation",
    "start_time": "time_validation",
    "timestamp": "time_validation",
    "fee": "fee",
    "reward": "reward",
    "gas": "gas",
    "version": "upgrade",
    "upgrade": "upgrade",
}


def iter_issue_chunks(text: str) -> List[str]:
    """
    æŒ‰ ID (å¦‚ A-1/M-3/L-5/H-1) ç²—åˆ†æŠ¥å‘Šæ®µè½ã€‚
    æ”¯æŒå¤šç§æ ¼å¼:
    - Format A (Cetus/FullSail): `A-1` ç›´æ¥åœ¨è¡Œé¦–
    - Format B (Scallop): `## H-1: Title` å¸¦ markdown æ ‡é¢˜
    """
    # å°è¯•ä¸¤ç§æ ¼å¼
    # Format B: Scallop æ ¼å¼ (## H-1: Title)
    pattern_scallop = re.compile(r"(?m)^##\s+([A-Z]-\d+):")
    matches_scallop = list(pattern_scallop.finditer(text))

    # Format A: æ™®é€šæ ¼å¼ (A-1 ç›´æ¥åœ¨è¡Œé¦–)
    pattern_simple = re.compile(r"(?m)^([A-Z]-\d+)\b")
    matches_simple = list(pattern_simple.finditer(text))

    # é€‰æ‹©åŒ¹é…æ•°æ›´å¤šçš„æ ¼å¼
    if len(matches_scallop) > len(matches_simple):
        matches = matches_scallop
        is_scallop = True
    else:
        matches = matches_simple
        is_scallop = False

    if not matches:
        return []

    chunks = []
    for idx, match in enumerate(matches):
        start = match.start()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
    return chunks


def extract_section(chunk: str, heading: str) -> Optional[str]:
    """
    æŠ½å–ä»¥ heading å¼€å¤´çš„æ®µè½æ–‡æœ¬ã€‚
    æ”¯æŒæ ¼å¼:
    - `Description\n...`
    - `### Description\n...`
    - `**Description**\n...`
    """
    # å°è¯•å¤šç§æ¨¡å¼
    patterns = [
        rf"(?i)###?\s*{heading}\s*\n",           # ### Description æˆ– ## Description
        rf"(?i)\*\*{heading}\*\*\s*\n",          # **Description**
        rf"(?i)^{heading}\s*\n",                 # Description (è¡Œé¦–)
        rf"(?i){heading}\s*\n",                  # Description (ä»»æ„ä½ç½®)
    ]

    match = None
    for pat in patterns:
        match = re.search(pat, chunk, re.MULTILINE)
        if match:
            break

    if not match:
        return None

    start = match.end()
    rest = chunk[start:].strip()
    lines: List[str] = []

    # åœæ­¢è¯ï¼šä¸‹ä¸€ä¸ªç« èŠ‚å¼€å§‹
    stop_words = {"description", "recommendation", "remediation", "severity", "keywords", "id"}

    for line in rest.splitlines():
        stripped = line.strip().lower()
        # æ£€æµ‹ä¸‹ä¸€ä¸ªç« èŠ‚ (### xxx æˆ– **xxx** æˆ–çº¯æ ‡é¢˜)
        if stripped in stop_words:
            break
        if re.match(r"^###?\s+\w+", line.strip()):
            break
        if re.match(r"^\*\*\w+.*\*\*$", line.strip()):
            break
        lines.append(line.rstrip())

    joined = "\n".join(lines).strip()
    return joined or None


def guess_severity(chunk: str) -> Optional[str]:
    """
    çŒœæµ‹ä¸¥é‡æ€§ã€‚æ”¯æŒæ ¼å¼:
    - `**Severity: High**`
    - `Severity\nhigh`
    - ç›´æ¥åŒ…å« critical/high/medium/low/advisory
    """
    # ä¼˜å…ˆåŒ¹é… **Severity: xxx** æ ¼å¼
    sev_match = re.search(r"\*\*Severity:\s*(\w+)\*\*", chunk, re.IGNORECASE)
    if sev_match:
        sev = sev_match.group(1).lower()
        if sev in SEVERITY_KEYWORDS:
            return sev

    # å¤‡é€‰ï¼šç›´æ¥æœç´¢å…³é”®è¯
    lower = chunk.lower()
    for sev in SEVERITY_KEYWORDS:
        if sev in lower:
            return sev
    return None


def find_issue_tags(chunk: str) -> List[str]:
    lower = chunk.lower()
    tags: Set[str] = set()
    for key, tag in KEYWORD_TAGS.items():
        if key in lower:
            tags.add(tag)
    return sorted(tags)


def find_detection_cues(chunk: str) -> List[str]:
    cues: Set[str] = set()
    # å‡½æ•°/æ¨¡å—å½¢å¼ï¼šmodule::function æˆ– foo.bar
    for match in re.findall(r"[A-Za-z_]+\:\:[A-Za-z0-9_]+", chunk):
        cues.add(match)
    for match in re.findall(r"[A-Za-z_]+\.[A-Za-z0-9_]+", chunk):
        cues.add(match)
    # å˜é‡/æ ‡è¯†ç¬¦çº¿ç´¢ï¼šå«ä¸‹åˆ’çº¿çš„è¯
    for match in re.findall(r"\b[a-z_]{3,}\b", chunk):
        if "_" in match and len(cues) < 15:
            cues.add(match)
    return sorted(cues)


def parse_chunk(chunk: str, source_file: str, project_name: str) -> Dict:
    """
    è§£æå•ä¸ªæ¼æ´ chunkã€‚æ”¯æŒæ ¼å¼:
    - Scallop: `## H-1: Title`
    - Cetus/FullSail: `A-1\nTitle\n...`
    """
    lines = [l.strip() for l in chunk.splitlines() if l.strip()]
    first_line = lines[0] if lines else ""

    # æå– ID å’Œ Title
    issue_id = "UNKNOWN"
    title = ""

    # Format B: Scallop æ ¼å¼ `## H-1: Title`
    scallop_match = re.match(r"^##\s+([A-Z]-\d+):\s*(.+)$", first_line)
    if scallop_match:
        issue_id = scallop_match.group(1)
        title = scallop_match.group(2).strip()
    else:
        # Format A: æ™®é€šæ ¼å¼
        id_match = re.match(r"^([A-Z]-\d+)\b", first_line)
        if id_match:
            issue_id = id_match.group(1)
        if len(lines) > 1 and not re.match(r"^[A-Z]-\d+$", lines[1]):
            title = lines[1]

    severity = guess_severity(chunk) or ""
    description = extract_section(chunk, "description") or ""
    recommendation = extract_section(chunk, "recommendation") or ""
    remediation = extract_section(chunk, "remediation") or ""

    # å¦‚æœæ²¡æœ‰æå–åˆ° descriptionï¼Œä½¿ç”¨æ•´ä¸ª chunk (å»æ‰æ ‡é¢˜è¡Œ)
    if not description:
        desc_lines = lines[1:] if scallop_match else lines[2:]
        description = "\n".join(desc_lines[:20])  # é™åˆ¶é•¿åº¦

    detection_cues = find_detection_cues(chunk)
    issue_tags = find_issue_tags(chunk)

    # å°è¯•ä» cues é‡Œæ‹†å‡ºç¬¬ä¸€ä¸ªå‡½æ•°å
    function_hint = None
    module_path = None
    for cue in detection_cues:
        if "::" in cue:
            function_hint = cue
            break

    # æå–ä¿®å¤çŠ¶æ€
    remediation_status = ""
    if "remediated" in chunk.lower():
        remediation_status = "remediated"
    elif "acknowledged" in chunk.lower():
        remediation_status = "acknowledged"
    elif "pending" in chunk.lower():
        remediation_status = "pending"

    return {
        "id": issue_id,
        "project": project_name,
        "source_file": source_file,
        "module_path": module_path,
        "function": function_hint,
        "title": title,
        "severity": severity,
        "issue_tags": issue_tags,
        "description": description[:2000],  # é™åˆ¶é•¿åº¦
        "recommendation": recommendation[:1000],
        "remediation_status": remediation_status,
        "detection_cues": detection_cues,
        "suggested_checks": [],
    }


def extract_from_file(md_path: Path) -> List[Dict]:
    text = md_path.read_text(encoding="utf-8", errors="ignore")
    chunks = iter_issue_chunks(text)
    project_name = md_path.stem
    patterns = [parse_chunk(chunk, md_path.name, project_name) for chunk in chunks]
    return patterns


def write_jsonl(output_path: Path, rows: List[Dict]) -> None:
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        for row in rows:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")


def build_vector_store(rows: List[Dict], vector_dir: Path, collection: str) -> None:
    if Chroma is None or Document is None or DashScopeEmbeddings is None:
        print("âš ï¸ æœªå®‰è£…å‘é‡åŒ–ä¾èµ–ï¼Œè·³è¿‡å‘é‡åº“ç”Ÿæˆã€‚")
        return
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("âš ï¸ æœªæ‰¾åˆ° DASHSCOPE_API_KEYï¼Œè·³è¿‡å‘é‡åº“ç”Ÿæˆã€‚")
        return

    docs: List[Document] = []
    for row in rows:
        # æ„å»ºæ–‡æ¡£å†…å®¹
        content = "\n".join(
            filter(
                None,
                [
                    row.get("title", ""),
                    row.get("description", ""),
                    row.get("recommendation", ""),
                    " | ".join(row.get("issue_tags", [])),
                ],
            )
        )

        # ChromaDB ä¸æ”¯æŒ list ç±»å‹çš„ metadataï¼Œéœ€è¦è½¬æ¢
        clean_metadata = {}
        for key, value in row.items():
            if isinstance(value, list):
                clean_metadata[key] = " | ".join(str(v) for v in value) if value else ""
            elif value is None:
                clean_metadata[key] = ""
            else:
                clean_metadata[key] = value

        docs.append(Document(page_content=content, metadata=clean_metadata))

    vector_dir.mkdir(parents=True, exist_ok=True)
    embeddings = DashScopeEmbeddings(model="text-embedding-v2", dashscope_api_key=api_key)
    Chroma.from_documents(
        documents=docs,
        embedding=embeddings,
        persist_directory=str(vector_dir),
        collection_name=collection,
    )
    print(f"âœ… å‘é‡åº“å·²ç”Ÿæˆ -> {vector_dir} (collection={collection})")


def main():
    parser = argparse.ArgumentParser(description="æå–å®‰å…¨/æ¼æ´æŠ¥å‘Šä¸ºç»“æ„åŒ–æ•°æ®é›†ï¼Œæ”¯æŒå¯é€‰å‘é‡åº“ã€‚")
    parser.add_argument("--input", type=Path, default=DEFAULT_INPUT_DIR, help="Markdown å®‰å…¨æŠ¥å‘Šç›®å½•")
    parser.add_argument("--out", type=Path, default=DEFAULT_OUTPUT, help="è¾“å‡º JSONL è·¯å¾„")
    parser.add_argument("--persist-vector", action="store_true", help="åŒæ—¶ç”Ÿæˆ security_patterns å‘é‡åº“")
    parser.add_argument("--vector-dir", type=Path, default=DEFAULT_VECTOR_DIR, help="å‘é‡åº“å­˜å‚¨ç›®å½•")
    parser.add_argument("--collection", type=str, default=DEFAULT_COLLECTION, help="å‘é‡åº“é›†åˆå")
    args = parser.parse_args()

    if not args.input.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input}")
        return

    md_files = sorted([p for p in args.input.glob("*.md") if p.is_file()])
    if not md_files:
        print(f"âŒ æœªæ‰¾åˆ° Markdown æ–‡ä»¶: {args.input}")
        return

    all_rows: List[Dict] = []
    for md_file in md_files:
        rows = extract_from_file(md_file)
        all_rows.extend(rows)
        print(f"âœ… è§£æ {md_file.name}: {len(rows)} æ¡")

    write_jsonl(args.out, all_rows)
    print(f"ğŸ¯ å·²å†™å…¥æ•°æ®é›†: {args.out} (å…± {len(all_rows)} æ¡)")

    if args.persist_vector:
        build_vector_store(all_rows, args.vector_dir, args.collection)


if __name__ == "__main__":
    main()
